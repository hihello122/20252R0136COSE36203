get_ipython().getoutput("pip install --upgrade pip wheel setuptools")

# Go ÏóÜÏù¥ ÏÑ§Ïπò Í∞ÄÎä•Ìïú ÏòàÏ†Ñ Î≤ÑÏ†Ñ ÏÇ¨Ïö©
get_ipython().getoutput("pip install "wandb<0.23"")
get_ipython().getoutput("pip install numpy")
get_ipython().getoutput("pip install pandas")
get_ipython().getoutput("pip install torch")
get_ipython().getoutput("pip install scikit-learn")
get_ipython().getoutput("pip install tqdm")
get_ipython().getoutput("pip install google")
get_ipython().getoutput("pip install google-genai pandas --upgrade")
get_ipython().getoutput("pip install s3fs")
get_ipython().getoutput("pip install transformers")
get_ipython().getoutput("pip install peft")









import os
import wandb

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset
from torch.utils.data import DataLoader, random_split

from sklearn.preprocessing import MinMaxScaler

from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"

print(device)


csv_filename = 'spy_2023_2024.csv'
csv_filepath = '/content/drive/MyDrive/2025 ML Project/datasets/spy_data.csv'

save_dir = 'prepared_data/'





# csv to DF
data = pd.read_csv(csv_filename)
data["date"] = pd.to_datetime(data["date"])


data


## ÌïòÎ£®Î•º Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÍ∑úÌôî
def normalize_per_day(group):
  ohlc = ['1. open', '2. high', '3. low', '4. close']

  min_val = group[ohlc].min().min()
  max_val = group[ohlc].max().max()

  if max_val - min_val > 0:
    group[ohlc] = (group[ohlc] - min_val) / (max_val - min_val)
  else:
    group[ohlc] = 0.5

  ## ÏùºÎã® volumeÎèÑ ÌïòÎ£® Îã®ÏúÑÎ°ú Ï†ïÍ∑úÌôî
  min_vol = group['5. volume'].min()
  max_vol = group['5. volume'].max()
  group['5. volume'] = (group['5. volume'] - min_vol) / (max_vol - min_vol)

  return group

normalized_data = data.groupby(data['date'].dt.date).apply(normalize_per_day)


normalized_data


normalized_data.head(14)


class SPYDataSet(Dataset):
  def __init__(self, data, features, chunk_size):
    self.chunk_size = chunk_size

    arr = data[features].to_numpy(dtype=np.float32)
    self.arr = arr
    self.N, self.C = arr.shape

    self.num_chunks = self.N // self.chunk_size

  def __len__(self):
    return self.num_chunks

  def __getitem__(self, idx: int):
    start = idx * self.chunk_size
    end = start + self.chunk_size

    x = self.arr[start:end]
    x = torch.from_numpy(x).float().T
    return {"x": x, "idx": idx}





feature_cols = ['1. open', '2. high', '3. low', '4. close', '5. volume']

ds = SPYDataSet(normalized_data, features=feature_cols, chunk_size=13)
print(len(ds))


ds[0] # C(5) * T(13)





class Encoder(nn.Module):
  def __init__(self, input_dim, hidden_dim, latent_dim):
    super().__init__()
    self.conv = nn.Sequential(
        nn.Conv1d(input_dim, hidden_dim, kernel_size=5, stride=2, padding=2),
        nn.ReLU(),
        nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=1, padding=2),
        nn.ReLU(),
        nn.Conv1d(hidden_dim, latent_dim, kernel_size=3, stride=1, padding=1),
        nn.ReLU(),
        nn.AdaptiveAvgPool1d(1)
    )

  def forward(self, x):
    return self.conv(x)


class Decoder(nn.Module):
  def __init__(self, latent_dim, hidden_dim, output_dim):
    super().__init__()
    self.deconv = nn.Sequential(
          nn.ConvTranspose1d(latent_dim, hidden_dim, kernel_size=7, stride=1, padding=0),
          nn.ReLU(),
          nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1),
          nn.ReLU(),
          nn.ConvTranspose1d(hidden_dim, hidden_dim, kernel_size=3, stride=2, padding=1, output_padding=0),
          nn.ReLU(),
          nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1),
          nn.ReLU(),
          nn.Conv1d(hidden_dim, output_dim, kernel_size=3, stride=1, padding=1)
      )

  def forward(self, z_q):
      return self.deconv(z_q)


class VectorQuantizer(nn.Module):
  """ num_embeddings: K (codebook size)
      embedding_dim:  D (code dimension)
      commitment_cost: beta in the paper """

  def __init__(self, num_embeddings, embedding_dim, commitment_cost):
    super().__init__()
    self.num_embeddings = num_embeddings
    self.embedding = nn.Embedding(num_embeddings, embedding_dim)
    self.commitment_cost = commitment_cost
    self.embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)

  def forward(self, z):
    B, D, T = z.shape
    z_perm = z.permute(0, 2, 1).contiguous()
    z_flattened = z_perm.view(-1, D)

    e = self.embedding.weight
    z_sq = (z_flattened ** 2).sum(dim=1, keepdim=True)
    e_sq = (e ** 2).sum(dim=1)
    ze = z_flattened @ e.t()
    distances = z_sq + e_sq.unsqueeze(0) - 2 * ze

    encoding_indices = torch.argmin(distances, dim=1)
    z_q = self.embedding(encoding_indices).view(B, T, D).permute(0, 2, 1).contiguous()

    codebook_loss =  F.mse_loss(z_q, z.detach())
    commitment_loss = self.commitment_cost * F.mse_loss(z_q.detach(), z)
    vq_loss = codebook_loss + 0.5 * commitment_loss

    z_q = z + (z_q - z).detach()

    indices_bt = encoding_indices.view(B, T)
    return z_q, vq_loss, indices_bt


class VQVAE(nn.Module):
  def __init__(self, input_dim, hidden_dim, latent_dim, num_embeddings, commitment_cost):
    super().__init__()
    self.encoder = Encoder(input_dim, hidden_dim, latent_dim)
    self.vq = VectorQuantizer(num_embeddings, latent_dim, commitment_cost)
    self.decoder = Decoder(latent_dim, hidden_dim, input_dim)

  def forward(self, x):
    z_e = self.encoder(x)
    z_q, vq_loss, indices = self.vq(z_e)
    x_recon = self.decoder(z_q)
    return x_recon, vq_loss, indices, z_q


def evaluate(model, dataloader, device):
  index_list = []
  sum_recon, sum_vq = 0.0, 0.0
  n = 0

  model.eval()
  with torch.no_grad():
    for batch in tqdm(dataloader, desc="Evaluating", leave=False):
      X = batch["x"].to(device)

      x_recon, vq_loss, indices, _ = model(X)

      recon_loss = F.mse_loss(x_recon, X)

      batch_size = X.size(0)
      sum_recon += recon_loss.item() * batch_size
      sum_vq += vq_loss.item() * batch_size
      n += batch_size

      index_list.append(indices)

    mean_recon = sum_recon / max(n, 1)
    mean_vq = sum_vq / max(n, 1)
    sum_loss = mean_recon + mean_vq

    return mean_recon, mean_vq, sum_loss, index_list


import wandb
print(wandb.__file__)


configs= [{
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 200,
        "num_embeddings": 16,
        "commitment_cost":0.25
    }, {
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 200,
        "num_embeddings": 16,
        "commitment_cost":0.4
    }, {
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 200,
        "num_embeddings": 32,
        "commitment_cost":0.25
    }, {
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 200,
        "num_embeddings": 32,
        "commitment_cost":0.4
    }, {
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 200,
        "num_embeddings": 64,
        "commitment_cost":0.25
    }, {
        "batch_size": 32,
        "learning_rate": 3e-4,
        "epochs": 200,
        "num_embeddings": 64,
        "commitment_cost":0.4
    }]



# legacy data load code
'''
N = len(ds)
train_len = int(N * 0.7)
val_len = int(N * 0.15)
test_len = N - train_len - val_len
   
ds_train, ds_val, ds_test = random_split(ds, [train_len, val_len, test_len])
train_loader = DataLoader(ds_train, batch_size=wandb.config.batch_size, shuffle=True, drop_last=True)
val_loader   = DataLoader(ds_val, batch_size=wandb.config.batch_size, shuffle=False)
test_loader  = DataLoader(ds_test, batch_size=wandb.config.batch_size, shuffle=False)
    
# parameter setting
'''


for i in range(len(configs)):
    cfg = configs[i]
    wandb.init(
        project="2025 ML Project",
        mode="offline",
        entity="youani-korea-university",
        name="1 vector_8",
        config= cfg
    )
    
    N = len(ds)
    train_len = int(N * 0.7)
    val_len = int(N * 0.15)
    test_len = N - train_len - val_len
    
    ds_train, ds_val, ds_test = random_split(ds, [train_len, val_len, test_len])
    train_loader = DataLoader(ds_train, batch_size=wandb.config.batch_size, shuffle=True, drop_last=True)
    val_loader   = DataLoader(ds_val, batch_size=wandb.config.batch_size, shuffle=False)
    test_loader  = DataLoader(ds_test, batch_size=wandb.config.batch_size, shuffle=False)
    
    # parameter setting
    input_dim = 5
    hidden_dim = 64
    latent_dim = 8
    num_embeddings = wandb.config.num_embeddings
    commitment_cost = wandb.config.commitment_cost
    
    # model training
    lr = wandb.config.learning_rate
    epochs = wandb.config.epochs
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = VQVAE(input_dim, hidden_dim, latent_dim, num_embeddings, commitment_cost)
    model.to(device)
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    recon_hist, vq_hist = [], []
    
    best_val_loss = float("inf")
    best_model_path = 'prepared_data/best_model_epoch.pt'
    
    for epoch in range(1, epochs + 1):
      model.train()
      sum_recon, sum_vq = 0.0, 0.0
      n = 0
    
      for batch in tqdm(train_loader, desc="Train", leave=False):
        X = batch['x'].to(device)
        opt.zero_grad()
        x_recon, vq_loss, indices, _ = model(X)
    
        if x_recon.size(-1) != X.size(-1):
          print("Error: reconstruction size not equal to original")
          x_recon = F.interpolate(x_recon, size=X.size(-1), mode='linear', align_corners=False)
    
        recon_loss = F.mse_loss(x_recon, X)
        loss = recon_loss + vq_loss
    
        loss.backward()
        opt.step()
    
        batch_size = X.size(0)
        sum_recon += recon_loss.item() * batch_size
        sum_vq += vq_loss.item() * batch_size
        n += batch_size

        
      epoch_recon = sum_recon / max(n, 1)
      epoch_vq = sum_vq / max(n, 1)
    
      recon_hist.append(epoch_recon)
      vq_hist.append(epoch_vq)
    
      tqdm.write(f"[{epoch:03d}/{epochs:03d} Training] recon={epoch_recon:.6f} vq={epoch_vq:.6f}")
    
      val_recon, val_vq, val_loss, index_list = evaluate(model, val_loader, device)
      index_total = torch.cat(index_list).view(-1)
      usage_rate = len(index_total.unique()) / model.vq.num_embeddings
      wandb.log({
          f'valid_epoch_recon': val_recon,
          f'valid_epoch_vq': val_vq,
          f'usage_rate': usage_rate
      })
      tqdm.write(f"[{epoch:03d}/{epochs:03d} Validation] recon={val_recon:.6f} vq={val_vq:.6f}")

      wandb.log({"train_loss": loss.item(), "vq_loss": vq_loss.item()})

    
      if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_path = os.path.join("model/vqvae/",f"num_embedding_{cfg['num_embeddings']}_commitment_cost_{cfg['commitment_cost']}.pt")
        torch.save(model.state_dict(),best_model_path)
    
    print(f"Training Finished. Best model: {best_model_path} (val_loss: {best_val_loss:.6f})")


input_dim = 5
hidden_dim = 64
latent_dim = 8

model_dirs = ["model/vqvae/num_embedding_16_commitment_cost_0.25.pt",
               "model/vqvae/num_embedding_16_commitment_cost_0.4.pt",
               "model/vqvae/num_embedding_32_commitment_cost_0.25.pt",
               "model/vqvae/num_embedding_32_commitment_cost_0.4.pt",
               "model/vqvae/num_embedding_64_commitment_cost_0.25.pt",
               "model/vqvae/num_embedding_64_commitment_cost_0.4.pt"]

for i in range(len(model_dirs)):
    cfg = configs[i]
    num_embeddings = cfg["num_embeddings"]
    commitment_cost = cfg["commitment_cost"]    
    
    model = VQVAE(input_dim, hidden_dim, latent_dim, num_embeddings, commitment_cost)
    model.to(device)

    model_path = model_dirs[i]
    model.load_state_dict(torch.load(model_path))
    print(model_path)
    wandb.init(
        project="2025 ML Project",
        mode="offline",
        entity="youani-korea-university",
        name="1 vector_8",
        config= cfg)

    test_recon, test_vq, test_loss, index_list = evaluate(model, test_loader, device)
    wandb.log({
        f'test_epoch_recon': test_recon,
        f'test_epoch_vq': test_vq
    })
    print(f"[Test] recon={test_recon:.6f} vq={test_vq:.6f}")



    # Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏö© DataLoader (shuffle=False ÌïÑÏàò)
    full_loader = DataLoader(
        ds,
        batch_size=wandb.config.batch_size,
        shuffle=False
    )

    model.eval()

    all_idx = []
    all_codes = []
    all_embs = []  # z_q Î≤°ÌÑ∞ (ÏòµÏÖò)

    with torch.no_grad():
        for batch in tqdm(full_loader, desc="Extract VQ codes"):
            X = batch["x"].to(device)          # (B, C, T)
            idxs = batch["idx"]                # Ï†ÑÏó≠ ÏúàÎèÑÏö∞ index

            x_recon, vq_loss, indices_bt, z_q = model(X)
            # indices_bt: (B, T=1) ‚Üí Í∞Å ÏúàÎèÑÏö∞Îãπ ÌïòÎÇòÏùò ÏΩîÎìú
            codes = indices_bt.squeeze(1).cpu().numpy()   # (B,)
            z_vec = z_q.squeeze(-1).cpu().numpy()         # (B, latent_dim)

            all_idx.extend(idxs.cpu().numpy().tolist())
            all_codes.extend(codes.tolist())
            all_embs.extend(z_vec.tolist())


    chunk_size = ds.chunk_size
    rows = []
    for idx, code, emb in zip(all_idx, all_codes, all_embs):
        start = idx * chunk_size
        end = start + chunk_size
        ts = normalized_data.iloc[end - 1]["date"]
        ts = ts.date()

        row = {"date": ts, "code": int(code)}
        for j, v in enumerate(emb):
            row[f"z_{j}"] = float(v)
        rows.append(row)
    
    df_vq = pd.DataFrame(rows).sort_values("date").reset_index(drop=True)

    # 2) Î°úÏª¨ ÏûÑÏãú ÌååÏùº Í≤ΩÎ°ú
    local_path = f"prepared_data/{i}th_embedding.csv"
    df_vq.to_csv(local_path, index=False, encoding="utf-8-sig")



model.load_state_dict(torch.load(best_model_path))
print(best_model_path)
wandb.init(
        project="2025 ML Project",
        mode="offline",
        entity="youani-korea-university",
        name="1 vector_8",
        config= cfg)

test_recon, test_vq, test_loss, index_list = evaluate(model, test_loader, device)
wandb.log({
    f'test_epoch_recon': test_recon,
    f'test_epoch_vq': test_vq
})
print(f"[Test] recon={test_recon:.6f} vq={test_vq:.6f}")



wandb.finish()





all_x = []
all_assign = []

model.eval()
with torch.no_grad():
  for batch in test_loader:
    X = batch["x"].to(device)
    _, _, indices, _ = model(X)

    all_x.append(X.cpu())
    all_assign.append(indices.cpu())

X = torch.cat(all_x, dim=0)
assign = torch.cat(all_assign, dim=0)
assign


## Embedding Extract


wandb.init(
        project="2025 ML Project",
        mode="offline",
        entity="youani-korea-university",
        name="1 vector_8",
        config= configs[0]
)


# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖãÏö© DataLoader (shuffle=False ÌïÑÏàò)
full_loader = DataLoader(
    ds,
    batch_size=wandb.config.batch_size,
    shuffle=False
)



model.eval()

all_idx = []
all_codes = []
all_embs = []  # z_q Î≤°ÌÑ∞ (ÏòµÏÖò)

with torch.no_grad():
    for batch in tqdm(full_loader, desc="Extract VQ codes"):
        X = batch["x"].to(device)          # (B, C, T)
        idxs = batch["idx"]                # Ï†ÑÏó≠ ÏúàÎèÑÏö∞ index

        x_recon, vq_loss, indices_bt, z_q = model(X)
        # indices_bt: (B, T=1) ‚Üí Í∞Å ÏúàÎèÑÏö∞Îãπ ÌïòÎÇòÏùò ÏΩîÎìú
        codes = indices_bt.squeeze(1).cpu().numpy()   # (B,)
        z_vec = z_q.squeeze(-1).cpu().numpy()         # (B, latent_dim)

        all_idx.extend(idxs.cpu().numpy().tolist())
        all_codes.extend(codes.tolist())
        all_embs.extend(z_vec.tolist())



chunk_size = ds.chunk_size
rows = []
for idx, code, emb in zip(all_idx, all_codes, all_embs):
    start = idx * chunk_size
    end = start + chunk_size
    ts = normalized_data.iloc[end - 1]["date"]
    ts = ts.date()

    row = {"date": ts, "code": int(code)}
    for j, v in enumerate(emb):
        row[f"z_{j}"] = float(v)
    rows.append(row)

df_vq = pd.DataFrame(rows).sort_values("date").reset_index(drop=True)

# 2) Î°úÏª¨ ÏûÑÏãú ÌååÏùº Í≤ΩÎ°ú
local_path = "prepared_data/embedding.csv"
df_vq.to_csv(local_path, index=False, encoding="utf-8-sig")



print(ds[1])


# One-cell: VQVAE inference -> latent ÏàòÏßë -> PCA / t-SNE Ï†ÄÏû• (Conv Íµ¨Ï°∞, input_dim=5)

'''
import os, re, warnings
import torch, torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

plt.switch_backend('Agg')  # ÌôîÎ©¥ Ï∂úÎ†• ÏóÜÏù¥ ÌååÏùº Ï†ÄÏû• Í∞ÄÎä•
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# -------------------------
# Î™®Îç∏ Ïª¥Ìè¨ÎÑåÌä∏ (ÌïôÏäµÏãú ÏÇ¨Ïö©Îêú Íµ¨Ï°∞ÏôÄ ÎèôÏùº)
# -------------------------
class Encoder(nn.Module):
    def __init__(self, input_dim=5, hidden_dim=64, latent_dim=8):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv1d(input_dim, hidden_dim, kernel_size=5, stride=2, padding=2),  # conv.0
            nn.ReLU(),                                                             # conv.1
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=1, padding=2), # conv.2
            nn.ReLU(),                                                             # conv.3
            nn.Conv1d(hidden_dim, latent_dim, kernel_size=3, stride=1, padding=1), # conv.4
            nn.ReLU(),                                                             # conv.5
        )
    def forward(self, x):
        return self.conv(x)  # [B, latent_dim, T]

class Decoder(nn.Module):
    def __init__(self, latent_dim=8, hidden_dim=64, output_dim=5):
        super().__init__()
        self.deconv = nn.Sequential(
            nn.ConvTranspose1d(latent_dim, hidden_dim, kernel_size=7, stride=1),       # deconv.0
            nn.ReLU(),                                                                 # deconv.1
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),               # deconv.2
            nn.ReLU(),                                                                 # deconv.3
            nn.ConvTranspose1d(hidden_dim, hidden_dim, kernel_size=3, stride=2, padding=1, output_padding=1),  # deconv.4
            nn.ReLU(),                                                                 # deconv.5
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),               # deconv.6
            nn.ReLU(),                                                                 # deconv.7
            nn.Conv1d(hidden_dim, output_dim, kernel_size=3, padding=1),               # deconv.8
        )
    def forward(self, z):
        return self.deconv(z)  # [B, output_dim, L']

class VectorQuantizer(nn.Module):
    def __init__(self, num_embeddings, embedding_dim, commitment_cost):
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.commitment_cost = commitment_cost
        self.embedding.weight.data.uniform_(-1.0/num_embeddings, 1.0/num_embeddings)

    def forward(self, z):
        # z: [B, D, T]
        B, D, T = z.shape
        z_perm = z.permute(0,2,1).contiguous()       # [B, T, D]
        z_flat = z_perm.view(-1, D)                  # [B*T, D]
        e = self.embedding.weight                    # [K, D]

        z_sq = (z_flat ** 2).sum(dim=1, keepdim=True)      # [B*T,1]
        e_sq = (e ** 2).sum(dim=1)                        # [K]
        ze = z_flat @ e.t()                               # [B*T, K]
        distances = z_sq + e_sq.unsqueeze(0) - 2 * ze     # [B*T, K]

        encoding_indices = torch.argmin(distances, dim=1) # [B*T]
        z_q = self.embedding(encoding_indices).view(B, T, D).permute(0,2,1).contiguous()  # [B, D, T]

        codebook_loss = F.mse_loss(z_q, z.detach())
        commitment_loss = self.commitment_cost * F.mse_loss(z_q.detach(), z)
        vq_loss = codebook_loss + 0.5 * commitment_loss

        z_q = z + (z_q - z).detach()  # straight-through
        indices_bt = encoding_indices.view(B, T)
        return z_q, vq_loss, indices_bt

class VQVAE(nn.Module):
    def __init__(self, input_dim=5, hidden_dim=64, latent_dim=8, num_embeddings=16, commitment_cost=0.25):
        super().__init__()
        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)
        self.vq = VectorQuantizer(num_embeddings, latent_dim, commitment_cost)
        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)
    def forward(self, x):
        z_e = self.encoder(x)
        z_q, vq_loss, indices = self.vq(z_e)
        x_recon = self.decoder(z_q)
        return x_recon, vq_loss, indices, z_q

# -------------------------
# Î™®Îç∏ Î™©Î°ù (ÎÑ§Í∞Ä Ï§Ä Í≤ΩÎ°ú)
# -------------------------
model_dirs = [
    "model/vqvae/num_embedding_16_commitment_cost_0.25.pt",
    "model/vqvae/num_embedding_16_commitment_cost_0.4.pt",
    "model/vqvae/num_embedding_32_commitment_cost_0.25.pt",
    "model/vqvae/num_embedding_32_commitment_cost_0.4.pt",
    "model/vqvae/num_embedding_64_commitment_cost_0.25.pt",
    "model/vqvae/num_embedding_64_commitment_cost_0.4.pt"
]

# -------------------------
# test loader Ï§ÄÎπÑ (ds_testÍ∞Ä Ïù¥ÎØ∏ ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ Dummy)
# -------------------------
try:
    ds_test  # Ï°¥Ïû¨ÌïòÎ©¥ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
    test_loader = DataLoader(ds_test, batch_size=64, shuffle=False)
    print("Using existing ds_test.")
except NameError:
    warnings.warn("ds_test not found; using DummyDataset (channels=5, length=48) for demonstration.")
    class DummyDataset(torch.utils.data.Dataset):
        def __init__(self, n=1024, channels=5, length=48):
            self.x = torch.randn(n, channels, length)
        def __len__(self): return len(self.x)
        def __getitem__(self, idx): return {"x": self.x[idx]}
    test_loader = DataLoader(DummyDataset(), batch_size=64, shuffle=False)

# -------------------------
# Ïú†Ìã∏: state_dict ÏïàÏ†Ñ Ï∂îÏ∂ú
# -------------------------
def extract_state_dict(loaded):
    # loaded = torch.load(...)
    if isinstance(loaded, dict):
        for k in ['state_dict', 'model_state', 'model', 'params', 'state', 'model_state_dict']:
            if k in loaded and isinstance(loaded[k], dict):
                return loaded[k]
        # maybe it's already a state_dict
        if all(isinstance(v, torch.Tensor) for v in loaded.values()):
            return loaded
    return loaded

# -------------------------
# Ïã§Ìñâ ÏòµÏÖò
# -------------------------
do_tsne = True
out_root = "plots_vqvae"
os.makedirs(out_root, exist_ok=True)

# -------------------------
# Î©îÏù∏ Î£®ÌîÑ
# -------------------------
for model_path in model_dirs:
    if not os.path.exists(model_path):
        warnings.warn(f"Model file not found: {model_path} ‚Äî skipping.")
        continue

    filename = os.path.basename(model_path)
    m = re.search(r"num_embedding_(\d+)_commitment_cost_([0-9.]+)\.pt$", filename)
    if not m:
        warnings.warn(f"Filename pattern mismatch: {filename} ‚Äî skipping.")
        continue

    num_embeddings = int(m.group(1))
    commitment_cost = float(m.group(2))
    print(f"\n=== Processing {filename} (emb={num_embeddings}, cc={commitment_cost}) ===")

    save_dir = os.path.join(out_root, f"emb_{num_embeddings}_cc_{commitment_cost}")
    os.makedirs(save_dir, exist_ok=True)

    # Í≥†Ï†ï ÌååÎùºÎØ∏ÌÑ∞(ÌïôÏäµ Ïãú ÏÇ¨Ïö©Ìïú Í∞í)
    input_dim = 5
    hidden_dim = 64
    latent_dim = 8

    # Î™®Îç∏ ÏÉùÏÑ±
    model = VQVAE(input_dim, hidden_dim, latent_dim, num_embeddings, commitment_cost).to(device)

    # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú (Îã§ÏñëÌïú Ìè¨Îß∑ ÎåÄÎπÑ)
    raw = torch.load(model_path, map_location=device)
    state = extract_state_dict(raw)

    # ÏãúÎèÑ 1: strict load
    loaded = False
    try:
        if isinstance(state, dict):
            model.load_state_dict(state)
        else:
            model.load_state_dict(raw)
        loaded = True
        print("Loaded state_dict (strict).")
    except Exception as e_strict:
        print("Strict load failed:", e_strict)
        # try removing module. prefix
        try:
            sd = state if isinstance(state, dict) else raw
            new_sd = {}
            for k,v in sd.items():
                new_k = k.replace("module.", "") if k.startswith("module.") else k
                new_sd[new_k] = v
            model.load_state_dict(new_sd, strict=False)
            loaded = True
            print("Loaded with strict=False after stripping 'module.' prefix (some keys may be missing).")
        except Exception as e_fallback:
            print("Fallback load failed:", e_fallback)
            raise RuntimeError(f"Failed to load state_dict for {model_path}.\n strict error: {e_strict}\n fallback error: {e_fallback}")

    model.eval()

    # latent ÏàòÏßë
    latents_list = []
    labels_list = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc=f"Collecting ({filename})"):
            X = batch["x"].to(device)
            if X.dim() == 2:  # [B, L] -> [B, 1, L]
                X = X.unsqueeze(1)
            # forward
            out = model(X)
            if isinstance(out, (list, tuple)) and len(out) >= 4:
                z_q = out[3]
            else:
                raise RuntimeError("Model forward did not return expected 4-tuple (x_recon, vq_loss, indices, z_q).")
            # z_q: [B, D, T]
            latents_list.append(z_q.detach().cpu())
            if isinstance(batch, dict) and 'label' in batch:
                labels_list.append(batch['label'].cpu().numpy())

    if len(latents_list) == 0:
        warnings.warn(f"No latents collected for {filename}.")
        continue

    latents = torch.cat(latents_list, dim=0)  # [N, D, T]
    N, D, T = latents.shape
    print(f"Collected latents: {latents.shape} -> flatten to [N, D*T]")

    latents_2d = latents.view(N, -1).numpy()  # [N, D*T]

    # PCA
    pca = PCA(n_components=2)
    latent_pca = pca.fit_transform(latents_2d)
    pca_path = os.path.join(save_dir, f"PCA_{filename.replace('.pt','')}.png")
    plt.figure(figsize=(7,6))
    plt.scatter(latent_pca[:,0], latent_pca[:,1], s=6, alpha=0.7)
    plt.title(f"PCA - {filename}")
    plt.xlabel("PC1"); plt.ylabel("PC2"); plt.tight_layout()
    plt.savefig(pca_path, dpi=300); plt.close()
    print(f"PCA saved: {pca_path}")

    # t-SNE (ÏòµÏÖò)
    if do_tsne:
        max_samples = 2000
        if N > max_samples:
            idx = np.random.choice(N, max_samples, replace=False)
            tsne_input = latents_2d[idx]
            print(f"t-SNE: sampling {max_samples}/{N} for speed.")
        else:
            tsne_input = latents_2d
            idx = None

        tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, max_iter=1000)
        latent_tsne = tsne.fit_transform(tsne_input)
        tsne_path = os.path.join(save_dir, f"TSNE_{filename.replace('.pt','')}.png")
        plt.figure(figsize=(7,6))
        plt.scatter(latent_tsne[:,0], latent_tsne[:,1], s=6, alpha=0.7)
        plt.title(f"t-SNE - {filename}")
        plt.tight_layout()
        plt.savefig(tsne_path, dpi=300); plt.close()
        print(f"t-SNE saved: {tsne_path}")

    # labeled plot if labels exist
    if len(labels_list) > 0:
        labels = np.concatenate(labels_list, axis=0)
        if idx is not None:
            plot_idx = idx
        else:
            plot_idx = np.arange(N)
        lab_path = os.path.join(save_dir, f"PCA_labeled_{filename.replace('.pt','')}.png")
        plt.figure(figsize=(7,6))
        plt.scatter(latent_pca[plot_idx,0], latent_pca[plot_idx,1], s=6, c=labels[plot_idx], cmap='coolwarm', alpha=0.7)
        plt.colorbar(); plt.tight_layout()
        plt.savefig(lab_path, dpi=300); plt.close()
        print(f"Labeled PCA saved: {lab_path}")

print("\nALL MODELS PROCESSED.")
'''





import pandas as pd
import json


embedding_path = "prepared_data/embedding.csv"
embedding_df = pd.read_csv(embedding_path)
embedding_df["date"] = pd.to_datetime(embedding_df["date"]).dt.date
embedding_df


news_path = "processed_news.csv"
news_df = pd.read_csv(news_path)
news_df["date"] = pd.to_datetime(news_df["date"]).dt.date
news_df


group_news_df = news_df.groupby("date")["text"].apply(list).reset_index()
group_news_df


group_news_df["text"] = group_news_df["text"].apply(json.dumps)
group_news_df


final_df = embedding_df.merge(group_news_df, on="date", how="inner")


final_df


final_path = "prepared_data/final.csv"
final_df.to_csv(final_path, index=False)


'''
for i in range(len(model_dirs)):
    embedding_path = f"prepared_data/{i}th_embedding.csv"
    embedding_df = pd.read_csv(embedding_path)
    embedding_df["date"] = pd.to_datetime(embedding_df["date"]).dt.date
    embedding_df

    news_path = "processed_news.csv"
    news_df = pd.read_csv(news_path)
    news_df["date"] = pd.to_datetime(news_df["date"]).dt.date
    news_df

    group_news_df = news_df.groupby("date")["text"].apply(list).reset_index()
    group_news_df

    group_news_df["text"] = group_news_df["text"].apply(json.dumps)
    group_news_df

    final_df = embedding_df.merge(group_news_df, on="date", how="inner")
    final_df

    final_path = f"prepared_data/{i}th_final.csv"
    final_df.to_csv(final_path, index=False)
'''





import pandas as pd
from google import genai
from google.genai.errors import APIError
import time

# --- 1. ÏÑ§Ï†ï Î∞è Ï¥àÍ∏∞Ìôî ---

# ‚ö†Ô∏è Ïó¨Í∏∞Ïóê Ïã§Ï†ú Gemini API ÌÇ§Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî.
# ÌôòÍ≤Ω Î≥ÄÏàòÎ•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏùÑ Í∂åÏû•ÌïòÏßÄÎßå, ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏßÅÏ†ë ÏûÖÎ†•Ìï† ÏàòÎèÑ ÏûàÏäµÎãàÎã§.
API_KEY = "AIzaSyDOX05SCJ5zcqWNlOs1gycY8bPNxW2RK6Q"
try:
    # client Í∞ùÏ≤¥Îäî for Î£®ÌîÑ Î∞ñÏóêÏÑú Ìïú Î≤àÎßå Ï¥àÍ∏∞ÌôîÎê©ÎãàÎã§.
    client = genai.Client(api_key=API_KEY)
    model = 'gemini-2.5-flash'  # Îπ†Î•∏ ÏùëÎãµÏùÑ ÏúÑÌï¥ flash Î™®Îç∏ ÏÇ¨Ïö©
except Exception as e:
    print(f"Gemini ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî Ïò§Î•ò: {e}")
    exit()

# --- 2. Îç∞Ïù¥ÌÑ∞ Î°úÎìú ---

file_path = 'prepared_data/final.csv'
try:
    df = pd.read_csv(file_path)
    print(f"‚úÖ ÌååÏùº '{file_path}' Î°úÎìú ÏôÑÎ£å. Ï¥ù {len(df)}Í∞ú Ìñâ.")
except FileNotFoundError:
    print(f"‚ùå Ïò§Î•ò: ÌååÏùº '{file_path}'Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Í≤ΩÎ°úÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.")
    exit()
except Exception as e:
    print(f"‚ùå Ïò§Î•ò: CSV ÌååÏùºÏùÑ ÏùΩÎäî Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}")
    exit()

# Í∑∏Î£πÌôîÌï† ÌÇ§ Ïª¨Îüº
GROUPING_KEY = 'code'

if GROUPING_KEY not in df.columns:
    print(f"‚ùå Ïò§Î•ò: Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê Í∑∏Î£πÌôî ÌÇ§Ïù∏ '{GROUPING_KEY}' Ïó¥Ïù¥ ÏóÜÏäµÎãàÎã§.")
    exit()

# --- 3. Silver Label ÏÉùÏÑ±ÏùÑ ÏúÑÌïú Ìï®Ïàò Ï†ïÏùò ---

def generate_silver_label(text_list, code_value, client_obj, model_name, prompt_input):
    """
    Ï£ºÏñ¥ÏßÑ ÌÖçÏä§Ìä∏ Î¶¨Ïä§Ìä∏Î•º Gemini APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏöîÏïΩÌï©ÎãàÎã§.
    (API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Í∞ùÏ≤¥Î•º Ïù∏ÏàòÎ°ú Î∞õÎèÑÎ°ù ÏàòÏ†ï)
    """
    if not text_list:
        return ""

    # ÌÖçÏä§Ìä∏Îì§ÏùÑ ÌïòÎÇòÏùò Î¨∏ÏûêÏó¥Î°ú Í≤∞Ìï©
    combined_text = "\n---\n".join(text_list)
    combined_text = combined_text[:100000]
    # ÌîÑÎ°¨ÌîÑÌä∏ Ï†ïÏùò
    prompt = f"""
    ÏïÑÎûòÏóê Ï†úÍ≥µÎêú ÌÖçÏä§Ìä∏Îì§ÏùÄ Í≥µÌÜµÏ†ÅÏúºÎ°ú ÏΩîÎìú Í∞í {code_value}Î•º Í∞ÄÏßÄÎäî Îâ¥Ïä§ Í∏∞ÏÇ¨/Ï†ïÎ≥¥Ïùò ÏùºÎ∂ÄÏûÖÎãàÎã§.
    
    Ïù¥ Î™®Îì† ÌÖçÏä§Ìä∏Ïùò ÌïµÏã¨ ÎÇ¥Ïö©Í≥º Ï£ºÏ†úÎ•º ÌååÏïÖÌïòÏó¨, 100~200Ïûê ÏÇ¨Ïù¥Ïùò ÌïòÎÇòÏùò **ÌïúÍµ≠Ïñ¥ ÏöîÏïΩ(Silver Label)**ÏúºÎ°ú Ï†ïÎ¶¨Ìï¥ Ï£ºÏÑ∏Ïöî.
    ÏöîÏïΩÏùÄ Îã§Ïùå ÌòïÏãùÏùÑ Îî∞Î¶ÖÎãàÎã§: "ÌïµÏã¨ Ï£ºÏ†ú: (ÌïµÏã¨ ÎÇ¥Ïö©)"
    
    --- ÌÖçÏä§Ìä∏ Î™©Î°ù ---
    {combined_text}
    """

    try:
        # Gemini API Ìò∏Ï∂ú
        response = client_obj.models.generate_content(
            model=model_name,
            contents=prompt
        )
        return response.text.strip()
    
    # ÌÜ†ÌÅ∞ ÎßåÎ£å ÎòêÎäî Í∏∞ÌÉÄ API Ïò§Î•ò Î∞úÏÉù Ïãú
    except APIError as e:
        # Ïò§Î•ò ÏÉÅÏÑ∏ Ï†ïÎ≥¥Î•º Î∞òÌôòÌïòÏó¨ ÎÇòÏ§ëÏóê ÏõêÏù∏ÏùÑ Î∂ÑÏÑùÌï† Ïàò ÏûàÍ≤å Ìï®
        return f"ÏöîÏïΩ Ïò§Î•ò: API Error - {e}"
    except Exception as e:
        return f"ÏöîÏïΩ Ïò§Î•ò: Unexpected Error - {e}"

# --- 4. Í∑∏Î£πÎ≥Ñ ÏöîÏïΩ Î∞è Silver Label ÏÉùÏÑ± (ÏßÄÏó∞/Î∞òÎ≥µÎ¨∏ Ï†ÅÏö©) ---

# GROUPING_KEYÎ•º Í∏∞Ï§ÄÏúºÎ°ú Í∑∏Î£πÌôî
grouped = df.groupby(GROUPING_KEY)['text'].apply(list).reset_index(name='text_list')
print(f"‚öôÔ∏è {GROUPING_KEY} Í∞í {len(grouped)}Í∞úÎ°ú Í∑∏Î£πÌôî ÏôÑÎ£å.")

# 'silver_label' Ïó¥ÏùÑ ÎØ∏Î¶¨ Ï¥àÍ∏∞ÌôîÌï©ÎãàÎã§.
grouped['silver_label'] = None 

print("‚ú® Í∑∏Î£πÎ≥Ñ Silver Label ÏÉùÏÑ± Ï§ë... (API Ìò∏Ï∂ú ÏÇ¨Ïù¥Ïóê **1Î∂Ñ ÏßÄÏó∞**Ïù¥ Ï†ÅÏö©Îê©ÎãàÎã§.)")

# Ïù∏Îç±Ïä§Î•º Î¶¨Ïä§Ìä∏Î°ú Î≥ÄÌôòÌïòÏó¨ ÎßàÏßÄÎßâ Ïù∏Îç±Ïä§Î•º ÏâΩÍ≤å ÌôïÏù∏
group_indices = grouped.index.tolist()
total_groups = len(group_indices)


prompts = [
  {
    "prompt_number": 1,
    "prompt_context": "ÏÉÅÏÑ∏ Î∂ÑÏÑù Î∞è Ìï≠Î™© Í∞ïÏ†ú: ÏöîÏïΩÏóê Î∞òÎìúÏãú 'Ï£ºÏöî ÏÇ∞ÏóÖ Î∂ÑÏïº/Í∏∞Ïà† Ìä∏Î†åÎìú', 'Ï£ºÏöî Ïù¥Ïäà/ÌôúÎèô Ïú†Ìòï', 'Ï†ÑÎ∞òÏ†ÅÏù∏ ÏãúÏû• Î∂ÑÏúÑÍ∏∞/ÏòÅÌñ•' ÏÑ∏ Í∞ÄÏßÄ ÏöîÏÜåÎ•º Ìè¨Ìï®ÌïòÎèÑÎ°ù Í∞ïÏ†úÌïòÏó¨ Íµ¨Ï≤¥Ï†ÅÏù∏ Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞Î•º ÌôïÎ≥¥Ìï©ÎãàÎã§.",
    "prompt_text": "ÏïÑÎûò Ï†úÍ≥µÎêú ÌÖçÏä§Ìä∏Îì§ÏùÄ Í≥µÌÜµÏ†ÅÏúºÎ°ú ÏΩîÎìú Í∞í {code_value}Î•º Í∞ÄÏßÄÎäî Îâ¥Ïä§ Í∏∞ÏÇ¨/Ï†ïÎ≥¥Ïùò ÏùºÎ∂ÄÏûÖÎãàÎã§.\n\nÎãπÏã†ÏùÄ Ï†ÑÎ¨∏ Îâ¥Ïä§ Î∂ÑÏÑùÍ∞ÄÏûÖÎãàÎã§. Ïù¥ Î™®Îì† ÌÖçÏä§Ìä∏Î•º Î∂ÑÏÑùÌïòÏó¨, Îã§Ïùå ÏÑ∏ Í∞ÄÏßÄ ÌïµÏã¨ Ìï≠Î™©ÏùÑ Î∞òÎìúÏãú Ìè¨Ìï®ÌïòÎäî 100~200Ïûê ÏÇ¨Ïù¥Ïùò ÌïòÎÇòÏùò **ÌïúÍµ≠Ïñ¥ ÏöîÏïΩ(Silver Label)**ÏúºÎ°ú Ï†ïÎ¶¨Ìï¥ Ï£ºÏÑ∏Ïöî.\n\n1. **Ïñ∏Í∏âÎêú Ï£ºÏöî ÏÇ∞ÏóÖ Î∂ÑÏïº ÎòêÎäî Í∏∞Ïà† Ìä∏Î†åÎìú** (Ïòà: IT, Ìó¨Ïä§ÏºÄÏñ¥, ESG, AI Îì±)\n2. **Ï£ºÏöî Ïù¥Ïäà/ÌôúÎèôÏùò Ïú†Ìòï** (Ïòà: Ïù∏ÏàòÌï©Î≥ë, Ïã†Ï†úÌíà Ï∂úÏãú, Ïã§Ï†Å Î∞úÌëú, Ï†ïÏ±Ö Î≥ÄÌôî Îì±)\n3. **Ï†ÑÎ∞òÏ†ÅÏù∏ ÏãúÏû• Î∂ÑÏúÑÍ∏∞ ÎòêÎäî ÏòÅÌñ•** (Ïòà: ÏÑ±Ïû• Í∏∞ÎåÄ, Î∂àÌôïÏã§ÏÑ± Ï¶ùÎåÄ, Í∑úÏ†ú Í∞ïÌôî Îì±)\n\nÏöîÏïΩÏùÄ Î∞òÎìúÏãú Îã§Ïùå ÌòïÏãùÏùÑ Îî∞Î•¥Î©∞, Íµ¨Ï≤¥Ï†ÅÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§:\n\"ÌïµÏã¨ Ï£ºÏ†ú: (ÌïµÏã¨ ÎÇ¥Ïö©. ÏúÑ 3Í∞ÄÏßÄ Ìï≠Î™©Ïù¥ Î™®Îëê ÎÖπÏïÑ ÏûàÏñ¥Ïïº Ìï®)\"\n\n--- ÌÖçÏä§Ìä∏ Î™©Î°ù ---\n{combined_text}",
    "output_name": "silver_label_detailed_analysis.csv"
  },
  {
    "prompt_number": 2,
    "prompt_context": "ÏãúÏû• ÎèôÌñ• Ï§ëÏã¨ ÏöîÏïΩ: Í∞úÎ≥Ñ Í∏∞ÏóÖ ÌôúÎèôÎ≥¥Îã§Îäî Í±∞ÏãúÏ†Å Í¥ÄÏ†êÏùò 'ÏãúÏû• Î≥ÄÌôî'ÏôÄ 'ÏúÑÌóò ÏöîÏÜå'Ïóê Ï¥àÏ†êÏùÑ ÎßûÏ∂∞ ÏöîÏïΩÌïòÎèÑÎ°ù Ïú†ÎèÑÌïòÏó¨ Ìä∏Î†åÎìú Î†àÏù¥Î∏îÏùÑ Ï∂îÏ∂úÌï©ÎãàÎã§.",
    "prompt_text": "ÏïÑÎûò Ï†úÍ≥µÎêú ÌÖçÏä§Ìä∏Îì§ÏùÄ Í≥µÌÜµÏ†ÅÏúºÎ°ú ÏΩîÎìú Í∞í {code_value}Î•º Í∞ÄÏßÄÎäî Îâ¥Ïä§ Í∏∞ÏÇ¨/Ï†ïÎ≥¥Ïùò ÏùºÎ∂ÄÏûÖÎãàÎã§.\n\nÏù¥ Î™®Îì† ÌÖçÏä§Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú, Ìï¥Îãπ ÏΩîÎìúÏôÄ Í¥ÄÎ†®Îêú **Í∞ÄÏû• Ï§ëÏöîÌïú Í±∞ÏãúÏ†Å ÏãúÏû• ÎèôÌñ•**Í≥º **Ïû†Ïû¨Ï†Å ÏúÑÌóò/Í∏∞Ìöå ÏöîÏÜå**Î•º 100~200Ïûê ÏÇ¨Ïù¥Ïùò ÌïòÎÇòÏùò **ÌïúÍµ≠Ïñ¥ ÏöîÏïΩ(Silver Label)**ÏúºÎ°ú Ï†ïÎ¶¨Ìï¥ Ï£ºÏÑ∏Ïöî.\n\n**[ÌïµÏã¨ ÏöîÍµ¨ÏÇ¨Ìï≠]**\n1. **Í≤ΩÏ†ú ÌôòÍ≤Ω/Ï†ïÏ±Ö Î≥ÄÌôî**Í∞Ä Ìè¨Ìï®ÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.\n2. **Í∞úÎ≥Ñ Í∏∞ÏóÖÎ™Ö Ïñ∏Í∏âÏùÄ ÏµúÏÜåÌôî**ÌïòÍ≥† ÏÇ∞ÏóÖ Ï†ÑÎ∞òÏùò Î∞©Ìñ•ÏÑ±ÏùÑ Ï†úÏãúÌï¥Ïïº Ìï©ÎãàÎã§.\n\nÏöîÏïΩ ÌòïÏãù: \"ÌïµÏã¨ Ï£ºÏ†ú: (ÏãúÏû• Î≥ÄÌôîÏôÄ Ï£ºÏöî Î¶¨Ïä§ÌÅ¨/Í∏∞ÌöåÏóê Ï¥àÏ†ê ÎßûÏ∂ò ÎÇ¥Ïö©)\"\n\n--- ÌÖçÏä§Ìä∏ Î™©Î°ù ---\n{combined_text}",
    "output_name": "silver_label_market_trend.csv"
  },
  {
    "prompt_number": 3,
    "prompt_context": "Í∞êÏÑ±/ÌÉúÎèÑ ÌèâÍ∞Ä: ÌÖçÏä§Ìä∏Ïùò Ï†ÑÎ∞òÏ†ÅÏù∏ Î∂ÑÏúÑÍ∏∞(Í∏çÏ†ï/Î∂ÄÏ†ï/Ï§ëÎ¶Ω)Î•º ÌååÏïÖÌïòÍ≥†, Í∑∏ Í∑ºÍ±∞Í∞Ä ÎêòÎäî ÌôúÎèô(Ìà¨Ïûê vs. ÌïòÎùΩ)ÏùÑ Î™ÖÏãúÌïòÎèÑÎ°ù ÏöîÍµ¨ÌïòÏó¨ Í∞êÏÑ± Îç∞Ïù¥ÌÑ∞ Î†àÏù¥Î∏îÏùÑ Ï∂îÏ∂úÌï©ÎãàÎã§.",
    "prompt_text": "ÏïÑÎûò Ï†úÍ≥µÎêú ÌÖçÏä§Ìä∏Îì§ÏùÑ Î∂ÑÏÑùÌïòÏó¨, Ìï¥Îãπ ÏΩîÎìú {code_value}ÏôÄ Í¥ÄÎ†®Îêú ÌôúÎèôÏùò **Ï†ÑÎ∞òÏ†ÅÏù∏ Í∞êÏÑ±(Í∏çÏ†ï/Î∂ÄÏ†ï/Ï§ëÎ¶Ω)**ÏùÑ ÌååÏïÖÌïòÍ≥† Í∑∏ Í∑ºÍ±∞Î•º ÏöîÏïΩÌïòÏÑ∏Ïöî.\n\nÏöîÏïΩÏùÄ Îã§Ïùå ÎÑ§ Í∞ÄÏßÄ ÏöîÏÜåÎ•º Î™®Îëê Ìè¨Ìï®ÌïòÎäî 100~200Ïûê ÏÇ¨Ïù¥Ïùò ÌïòÎÇòÏùò **ÌïúÍµ≠Ïñ¥ ÏöîÏïΩ(Silver Label)**ÏúºÎ°ú Ï†ïÎ¶¨Ìï¥ Ï£ºÏÑ∏Ïöî.\n\n1. **Í∞ÄÏû• ÎπàÎ≤àÌïòÍ≤å Ïñ∏Í∏âÎêú Í∏∞ÏóÖ ÌôúÎèô** (Ïòà: ÏÑ±Ïû•, Ìà¨Ïûê, ÌïòÎùΩ, Íµ¨Ï°∞Ï°∞Ï†ï)\n2. **Ï†ÑÎ∞òÏ†ÅÏù∏ Í∞êÏÑ± ÌèâÍ∞Ä** (Í∏çÏ†ï, Î∂ÄÏ†ï, ÎòêÎäî ÌòºÏû¨)\n3. **Í∞êÏÑ± ÌèâÍ∞ÄÏùò Í∑ºÍ±∞**Í∞Ä ÎêòÎäî Ï£ºÏöî ÏÇ¨Í±¥ (Í∏çÏ†ïÏ†Å/Î∂ÄÏ†ïÏ†Å)\n\nÏöîÏïΩ ÌòïÏãù: \"ÌïµÏã¨ Ï£ºÏ†ú: (Ï†ÑÎ∞òÏ†ÅÏù∏ Í∞êÏÑ± ÌèâÍ∞ÄÏôÄ Í∑∏ Í∑ºÍ±∞Î•º Î™ÖÌôïÌûà Ï†úÏãúÌïú ÎÇ¥Ïö©)\"\n\n--- ÌÖçÏä§Ìä∏ Î™©Î°ù ---\n{combined_text}",
    "output_name": "silver_label_sentiment_focus.csv"
  }
]

for index, p in enumerate(prompts):
    concated_prompt = p["prompt_context"] + p["prompt_text"]

# Î∞òÎ≥µÎ¨∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ìïú Ï§ÑÏî© Ï≤òÎ¶¨
    for i, index in enumerate(group_indices):
        row = grouped.loc[index]
        text_list = row['text_list']
        group_key = row[GROUPING_KEY]
        
        print(f"\nüöÄ {i+1}/{total_groups} Ï≤òÎ¶¨ Ï§ë - Code: {group_key} (ÌÖçÏä§Ìä∏ {len(text_list)}Í∞ú)")

        # 1. generate_silver_label Ìï®Ïàò Ìò∏Ï∂ú
        silver_label = generate_silver_label(text_list, group_key, client, model, concated_prompt)
        
        # 2. Í≤∞Í≥º Ï†ÄÏû•
        grouped.loc[index, 'silver_label'] = silver_label
    
        if silver_label.startswith("ÏöîÏïΩ Ïò§Î•ò:"):
            print(f"‚ùå Ïò§Î•ò Î∞úÏÉù: {silver_label}")
        else:
            print("‚úÖ Silver Label ÏÉùÏÑ± Î∞è Ï†ÄÏû• ÏôÑÎ£å.")

        time.sleep(0.5)
        
    print("\n‚úÖ Silver Label ÏÉùÏÑ± ÏôÑÎ£å.")

    # --- 5. ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê Silver Label Î≥ëÌï© ---

    # Î≥ëÌï©ÏùÑ ÏúÑÌï¥ 'silver_label'Í≥º GROUPING_KEYÎßå Ìè¨Ìï®ÌïòÎäî Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ Ï§ÄÎπÑ
    silver_labels_df = grouped[[GROUPING_KEY, 'silver_label']]

    # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê Î≥ëÌï©
    df_merged = pd.merge(df.drop(columns=['text']), silver_labels_df, on=GROUPING_KEY, how='left')

    # 'silver_label'ÏùÑ ÏÉà 'text'Î°ú ÏÇ¨Ïö©ÌïòÍ≥† Ïù¥Î¶ÑÏùÑ Î≥ÄÍ≤Ω
    df_merged.rename(columns={'silver_label': 'text'}, inplace=True)

    # Ïó¥ ÏàúÏÑú Ï°∞Ï†ï (ÏõêÎ≥∏Í≥º Ïú†ÏÇ¨ÌïòÍ≤å)
    cols = list(df_merged.columns)
    cols.remove('text')
    df_final = df_merged[cols + ['text']]

    # --- 6. Í≤∞Í≥º Ï†ÄÏû• ---

    output_file_path = p["output_name"]
    try:
        df_final.to_csv(output_file_path, index=False, encoding='utf-8')
        print(f"\nüéâ ÏµúÏ¢Ö Í≤∞Í≥ºÍ∞Ä '{output_file_path}'Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.")
        print("üëâ Ïù¥Ï†ú Ïù¥ ÌååÏùºÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌõÑÏÜç Î∂ÑÏÑùÏùÑ ÏßÑÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§.")
    except Exception as e:
        print(f"‚ùå Ïò§Î•ò: ÏµúÏ¢Ö CSV ÌååÏùº Ï†ÄÏû• Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}")


'''
import pandas as pd
from google import genai
from google.genai.errors import APIError
import time

# --- 1. ÏÑ§Ï†ï Î∞è Ï¥àÍ∏∞Ìôî ---

# ‚ö†Ô∏è Ïó¨Í∏∞Ïóê Ïã§Ï†ú Gemini API ÌÇ§Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî.
# ÌôòÍ≤Ω Î≥ÄÏàòÎ•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏùÑ Í∂åÏû•ÌïòÏßÄÎßå, ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ ÏßÅÏ†ë ÏûÖÎ†•Ìï† ÏàòÎèÑ ÏûàÏäµÎãàÎã§.
API_KEY = "google-gemini-API"
try:
    # client Í∞ùÏ≤¥Îäî for Î£®ÌîÑ Î∞ñÏóêÏÑú Ìïú Î≤àÎßå Ï¥àÍ∏∞ÌôîÎê©ÎãàÎã§.
    client = genai.Client(api_key=API_KEY)
    model = 'gemini-2.5-flash'  # Îπ†Î•∏ ÏùëÎãµÏùÑ ÏúÑÌï¥ flash Î™®Îç∏ ÏÇ¨Ïö©
except Exception as e:
    print(f"Gemini ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî Ïò§Î•ò: {e}")
    exit()

# --- 2. Îç∞Ïù¥ÌÑ∞ Î°úÎìú ---

for ablation in range(6):
    
    file_path = f'prepared_data/{ablation}th_final.csv'
    try:
        df = pd.read_csv(file_path)
        print(f"‚úÖ ÌååÏùº '{file_path}' Î°úÎìú ÏôÑÎ£å. Ï¥ù {len(df)}Í∞ú Ìñâ.")
    except FileNotFoundError:
        print(f"‚ùå Ïò§Î•ò: ÌååÏùº '{file_path}'Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Í≤ΩÎ°úÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.")
        exit()
    except Exception as e:
        print(f"‚ùå Ïò§Î•ò: CSV ÌååÏùºÏùÑ ÏùΩÎäî Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}")
        exit()

    # Í∑∏Î£πÌôîÌï† ÌÇ§ Ïª¨Îüº
    GROUPING_KEY = 'code'

    if GROUPING_KEY not in df.columns:
        print(f"‚ùå Ïò§Î•ò: Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê Í∑∏Î£πÌôî ÌÇ§Ïù∏ '{GROUPING_KEY}' Ïó¥Ïù¥ ÏóÜÏäµÎãàÎã§.")
        exit()

    # --- 3. Silver Label ÏÉùÏÑ±ÏùÑ ÏúÑÌïú Ìï®Ïàò Ï†ïÏùò ---

    def generate_silver_label(text_list, code_value, client_obj, model_name, prompt_input):
        """
        Ï£ºÏñ¥ÏßÑ ÌÖçÏä§Ìä∏ Î¶¨Ïä§Ìä∏Î•º Gemini APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏöîÏïΩÌï©ÎãàÎã§.
        (API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Í∞ùÏ≤¥Î•º Ïù∏ÏàòÎ°ú Î∞õÎèÑÎ°ù ÏàòÏ†ï)
        """
        if not text_list:
            return ""

        # ÌÖçÏä§Ìä∏Îì§ÏùÑ ÌïòÎÇòÏùò Î¨∏ÏûêÏó¥Î°ú Í≤∞Ìï©
        combined_text = "\n---\n".join(text_list)
        combined_text = combined_text[:100000]
        # ÌîÑÎ°¨ÌîÑÌä∏ Ï†ïÏùò
        prompt = f"""
        ÏïÑÎûòÏóê Ï†úÍ≥µÎêú ÌÖçÏä§Ìä∏Îì§ÏùÄ Í≥µÌÜµÏ†ÅÏúºÎ°ú ÏΩîÎìú Í∞í {code_value}Î•º Í∞ÄÏßÄÎäî Îâ¥Ïä§ Í∏∞ÏÇ¨/Ï†ïÎ≥¥Ïùò ÏùºÎ∂ÄÏûÖÎãàÎã§.

        Ïù¥ Î™®Îì† ÌÖçÏä§Ìä∏Ïùò ÌïµÏã¨ ÎÇ¥Ïö©Í≥º Ï£ºÏ†úÎ•º ÌååÏïÖÌïòÏó¨, 100~200Ïûê ÏÇ¨Ïù¥Ïùò ÌïòÎÇòÏùò **ÌïúÍµ≠Ïñ¥ ÏöîÏïΩ(Silver Label)**ÏúºÎ°ú Ï†ïÎ¶¨Ìï¥ Ï£ºÏÑ∏Ïöî.
        ÏöîÏïΩÏùÄ Îã§Ïùå ÌòïÏãùÏùÑ Îî∞Î¶ÖÎãàÎã§: "ÌïµÏã¨ Ï£ºÏ†ú: (ÌïµÏã¨ ÎÇ¥Ïö©)"

        --- ÌÖçÏä§Ìä∏ Î™©Î°ù ---
        {combined_text}
        """

        try:
            # Gemini API Ìò∏Ï∂ú
            response = client_obj.models.generate_content(
                model=model_name,
                contents=prompt
            )
            return response.text.strip()

        # ÌÜ†ÌÅ∞ ÎßåÎ£å ÎòêÎäî Í∏∞ÌÉÄ API Ïò§Î•ò Î∞úÏÉù Ïãú
        except APIError as e:
            # Ïò§Î•ò ÏÉÅÏÑ∏ Ï†ïÎ≥¥Î•º Î∞òÌôòÌïòÏó¨ ÎÇòÏ§ëÏóê ÏõêÏù∏ÏùÑ Î∂ÑÏÑùÌï† Ïàò ÏûàÍ≤å Ìï®
            return f"ÏöîÏïΩ Ïò§Î•ò: API Error - {e}"
        except Exception as e:
            return f"ÏöîÏïΩ Ïò§Î•ò: Unexpected Error - {e}"

    # --- 4. Í∑∏Î£πÎ≥Ñ ÏöîÏïΩ Î∞è Silver Label ÏÉùÏÑ± (ÏßÄÏó∞/Î∞òÎ≥µÎ¨∏ Ï†ÅÏö©) ---

    # GROUPING_KEYÎ•º Í∏∞Ï§ÄÏúºÎ°ú Í∑∏Î£πÌôî
    grouped = df.groupby(GROUPING_KEY)['text'].apply(list).reset_index(name='text_list')
    print(f"‚öôÔ∏è {GROUPING_KEY} Í∞í {len(grouped)}Í∞úÎ°ú Í∑∏Î£πÌôî ÏôÑÎ£å.")

    # 'silver_label' Ïó¥ÏùÑ ÎØ∏Î¶¨ Ï¥àÍ∏∞ÌôîÌï©ÎãàÎã§.
    grouped['silver_label'] = None 

    print("‚ú® Í∑∏Î£πÎ≥Ñ Silver Label ÏÉùÏÑ± Ï§ë... (API Ìò∏Ï∂ú ÏÇ¨Ïù¥Ïóê **1Î∂Ñ ÏßÄÏó∞**Ïù¥ Ï†ÅÏö©Îê©ÎãàÎã§.)")

    # Ïù∏Îç±Ïä§Î•º Î¶¨Ïä§Ìä∏Î°ú Î≥ÄÌôòÌïòÏó¨ ÎßàÏßÄÎßâ Ïù∏Îç±Ïä§Î•º ÏâΩÍ≤å ÌôïÏù∏
    group_indices = grouped.index.tolist()
    total_groups = len(group_indices)


    prompts = [
      {
    "prompt_number": 1,
    "prompt_context": "ÏÉÅÏÑ∏ Î∂ÑÏÑù Î∞è Ìï≠Î™© Í∞ïÏ†ú: ÏöîÏïΩÏóê Î∞òÎìúÏãú 'Ï£ºÏöî ÏÇ∞ÏóÖ Î∂ÑÏïº/Í∏∞Ïà† Ìä∏Î†åÎìú', 'Ï£ºÏöî Ïù¥Ïäà/ÌôúÎèô Ïú†Ìòï', 'Ï†ÑÎ∞òÏ†ÅÏù∏ ÏãúÏû• Î∂ÑÏúÑÍ∏∞/ÏòÅÌñ•' ÏÑ∏ Í∞ÄÏßÄ ÏöîÏÜåÎ•º Ìè¨Ìï®ÌïòÎèÑÎ°ù Í∞ïÏ†úÌïòÏó¨ Íµ¨Ï≤¥Ï†ÅÏù∏ Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞Î•º ÌôïÎ≥¥Ìï©ÎãàÎã§.",
    "prompt_text": "ÏïÑÎûò Ï†úÍ≥µÎêú ÌÖçÏä§Ìä∏Îì§ÏùÄ Í≥µÌÜµÏ†ÅÏúºÎ°ú ÏΩîÎìú Í∞í {code_value}Î•º Í∞ÄÏßÄÎäî Îâ¥Ïä§ Í∏∞ÏÇ¨/Ï†ïÎ≥¥Ïùò ÏùºÎ∂ÄÏûÖÎãàÎã§.\n\nÎãπÏã†ÏùÄ Ï†ÑÎ¨∏ Îâ¥Ïä§ Î∂ÑÏÑùÍ∞ÄÏûÖÎãàÎã§. Ïù¥ Î™®Îì† ÌÖçÏä§Ìä∏Î•º Î∂ÑÏÑùÌïòÏó¨, Îã§Ïùå ÏÑ∏ Í∞ÄÏßÄ ÌïµÏã¨ Ìï≠Î™©ÏùÑ Î∞òÎìúÏãú Ìè¨Ìï®ÌïòÎäî 100~200Ïûê ÏÇ¨Ïù¥Ïùò ÌïòÎÇòÏùò **ÌïúÍµ≠Ïñ¥ ÏöîÏïΩ(Silver Label)**ÏúºÎ°ú Ï†ïÎ¶¨Ìï¥ Ï£ºÏÑ∏Ïöî.\n\n1. **Ïñ∏Í∏âÎêú Ï£ºÏöî ÏÇ∞ÏóÖ Î∂ÑÏïº ÎòêÎäî Í∏∞Ïà† Ìä∏Î†åÎìú** (Ïòà: IT, Ìó¨Ïä§ÏºÄÏñ¥, ESG, AI Îì±)\n2. **Ï£ºÏöî Ïù¥Ïäà/ÌôúÎèôÏùò Ïú†Ìòï** (Ïòà: Ïù∏ÏàòÌï©Î≥ë, Ïã†Ï†úÌíà Ï∂úÏãú, Ïã§Ï†Å Î∞úÌëú, Ï†ïÏ±Ö Î≥ÄÌôî Îì±)\n3. **Ï†ÑÎ∞òÏ†ÅÏù∏ ÏãúÏû• Î∂ÑÏúÑÍ∏∞ ÎòêÎäî ÏòÅÌñ•** (Ïòà: ÏÑ±Ïû• Í∏∞ÎåÄ, Î∂àÌôïÏã§ÏÑ± Ï¶ùÎåÄ, Í∑úÏ†ú Í∞ïÌôî Îì±)\n\nÏöîÏïΩÏùÄ Î∞òÎìúÏãú Îã§Ïùå ÌòïÏãùÏùÑ Îî∞Î•¥Î©∞, Íµ¨Ï≤¥Ï†ÅÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§:\n\"ÌïµÏã¨ Ï£ºÏ†ú: (ÌïµÏã¨ ÎÇ¥Ïö©. ÏúÑ 3Í∞ÄÏßÄ Ìï≠Î™©Ïù¥ Î™®Îëê ÎÖπÏïÑ ÏûàÏñ¥Ïïº Ìï®)\"\n\n--- ÌÖçÏä§Ìä∏ Î™©Î°ù ---\n{combined_text}",
    "output_name": "silver_label_detailed_analysis.csv"
      }
    ]

    for index, p in enumerate(prompts):
        concated_prompt = p["prompt_context"] + p["prompt_text"]

    # Î∞òÎ≥µÎ¨∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ìïú Ï§ÑÏî© Ï≤òÎ¶¨
        for i, index in enumerate(group_indices):
            row = grouped.loc[index]
            text_list = row['text_list']
            group_key = row[GROUPING_KEY]

            print(f"\nüöÄ {i+1}/{total_groups} Ï≤òÎ¶¨ Ï§ë - Code: {group_key} (ÌÖçÏä§Ìä∏ {len(text_list)}Í∞ú)")

            # 1. generate_silver_label Ìï®Ïàò Ìò∏Ï∂ú
            silver_label = generate_silver_label(text_list, group_key, client, model, concated_prompt)

            # 2. Í≤∞Í≥º Ï†ÄÏû•
            grouped.loc[index, 'silver_label'] = silver_label

            if silver_label.startswith("ÏöîÏïΩ Ïò§Î•ò:"):
                print(f"‚ùå Ïò§Î•ò Î∞úÏÉù: {silver_label}")
            else:
                print("‚úÖ Silver Label ÏÉùÏÑ± Î∞è Ï†ÄÏû• ÏôÑÎ£å.")

            time.sleep(0.5)

        print("\n‚úÖ Silver Label ÏÉùÏÑ± ÏôÑÎ£å.")

        # --- 5. ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê Silver Label Î≥ëÌï© ---

        # Î≥ëÌï©ÏùÑ ÏúÑÌï¥ 'silver_label'Í≥º GROUPING_KEYÎßå Ìè¨Ìï®ÌïòÎäî Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ Ï§ÄÎπÑ
        silver_labels_df = grouped[[GROUPING_KEY, 'silver_label']]

        # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê Î≥ëÌï©
        df_merged = pd.merge(df.drop(columns=['text']), silver_labels_df, on=GROUPING_KEY, how='left')

        # 'silver_label'ÏùÑ ÏÉà 'text'Î°ú ÏÇ¨Ïö©ÌïòÍ≥† Ïù¥Î¶ÑÏùÑ Î≥ÄÍ≤Ω
        df_merged.rename(columns={'silver_label': 'text'}, inplace=True)

        # Ïó¥ ÏàúÏÑú Ï°∞Ï†ï (ÏõêÎ≥∏Í≥º Ïú†ÏÇ¨ÌïòÍ≤å)
        cols = list(df_merged.columns)
        cols.remove('text')
        df_final = df_merged[cols + ['text']]

        # --- 6. Í≤∞Í≥º Ï†ÄÏû• ---

        output_file_path = f"{ablation}th_" + p["output_name"]
        try:
            df_final.to_csv(output_file_path, index=False, encoding='utf-8')
            print(f"\nüéâ ÏµúÏ¢Ö Í≤∞Í≥ºÍ∞Ä '{output_file_path}'Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.")
            print("üëâ Ïù¥Ï†ú Ïù¥ ÌååÏùºÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÌõÑÏÜç Î∂ÑÏÑùÏùÑ ÏßÑÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§.")
        except Exception as e:
            print(f"‚ùå Ïò§Î•ò: ÏµúÏ¢Ö CSV ÌååÏùº Ï†ÄÏû• Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§: {e}")
'''





import os
import math
from typing import List

import numpy as np
import pandas as pd

print("pandas version:", pd.__version__)

# Data Path
OUTPUT_JSONL_LOCAL_PATH = "prepared_data/train.jsonl"
SILVER_LABEL_PATH = "silver_label_detailed_analysis.csv"
PRICE_OHLCV_PATH = "spy_2023_2024.csv"


df_silver_label = pd.read_csv(SILVER_LABEL_PATH)
df_price = pd.read_csv(PRICE_OHLCV_PATH)


# required_cols = {"date", "close"}
# if not required_cols.issubset(df_price.columns):
# raise ValueError("prices_ohlcv.csvÏóê date, close Ïª¨ÎüºÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.")

df_silver_label['date'] = pd.to_datetime(df_silver_label['date']).dt.date

df_price['date'] = pd.to_datetime(df_price['date'])
df_price['day'] = df_price['date'].dt.date
df_price = df_price.groupby('day').tail(1)

def make_summary(row):
    return (
        f"open: {row['1. open']}, "
        f"high: {row['2. high']}, "
        f"low: {row['3. low']}, "
        f"close: {row['4. close']}, "
        f"volume: {row['5. volume']}"
    )

df_price["ohlcv_summary"] = df_price.apply(make_summary, axis=1)
df_price = df_price[['day', 'ohlcv_summary']].copy()
df_price = df_price.rename(columns={'day': 'date'})
df_price['date'] = pd.to_datetime(df_price['date']).dt.date

# df_price["date"] = pd.to_datetime(df_price["date"])
# df_price = df_price.sort_values("date").reset_index(drop=True)

# Ïó¨Í∏∞ÏÑú ÏàòÏùµÎ•†, Î≥ÄÎèôÏÑ± Îì± ÌååÏÉù ÌîºÏ≤ò ÏÉùÏÑ±
# df_price["ret_1d"] = df_price["close"].pct_change()
# df_price["log_ret"] = np.log(df_price["close"]).diff()
# df_price["close_ma_5"] = df_price["close"].rolling(window=5).mean()
# df_price["close_ma_20"] = df_price["close"].rolling(window=20).mean()
# df_price["vol_5"] = df_price["log_ret"].rolling(window=5).std()
# df_price["vol_20"] = df_price["log_ret"].rolling(window=20).std()

# NaN Ï†úÍ±∞
# df_price = df_price.dropna().reset_index(drop=True)
# print("[PRICE] head:\n", df_price.head())


# VQ-VAE ÏΩîÎìúÏôÄ price join
# df_vq = pd.read_csv(VQ_CODE_CSV_PATH)
# df_vq["date"] = pd.to_datetime(df_vq["date"])

# df_merged = pd.merge(df_vq, df_price, on="date", how="inner")
# df_merged = df_merged.sort_values("date").reset_index(drop=True)

# News Headline join
# df_news = pd.read_csv("NEWS_CSV_PATH")
# df_news["Date"] = pd.to_datetime(df_news["Date"])

# merge all
df_all = pd.merge(
    df_silver_label,
    df_price,                # VQ ÏΩîÎìú + Í∞ÄÍ≤© ÏöîÏïΩ
    on="date",
    how="left",
)

df_price.to_csv("prepared_data/df_price_with_summary.csv", index=False)


def code_to_str(row):
    z_values = [row[f"z_{i}"] for i in range(8)]
    z_str = ", ".join([f"{v:.6f}" for v in z_values])
    return f"{z_str}"

df_all["code_str"] = df_all.apply(code_to_str, axis=1)


df_all.head()


def build_prompt(code_value: int, code_vector: str, ohlcv_summary: str):
    prompt = (
        "### Ï£ºÏñ¥ÏßÑ Ï†ïÎ≥¥\n ÏïÑÎûòÎäî ÌäπÏ†ï Í∏∞Í∞Ñ ÎèôÏïàÏùò ÏãúÏû• Ìå®ÌÑ¥ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî VQ ÏΩîÎìúÏôÄ Ìï¥Îãπ Í∏∞Í∞ÑÏùò OHLCV Îç∞Ïù¥ÌÑ∞Ïù¥Îã§.\n\n"
        "### ÏΩîÎìúÎ∂Å\n" + str(code_value) +"\n\n"
        "### ÏΩîÎìúÎ∂Å Î≤°ÌÑ∞\n" + str(code_vector) + "\n\n"
        "### OHLCV ÏãúÍ≥ÑÏó¥\n" + ohlcv_summary + "\n\n"
        
        """
        ### ÏßÄÏãúÎ¨∏\n ÏúÑÏùò Îç∞Ïù¥ÌÑ∞Í∞Ä ÎÇòÌÉÄÎÇ¥Îäî ÏãúÏû• ÏÉÅÌô©ÏùÑ Î∂ÑÏÑùÌïòÏó¨, Îã§Ïùå Ìï≠Î™©ÏùÑ Ìè¨Ìï®Ìïú Î∂ÑÏÑù ÏöîÏïΩÏùÑ ÏûëÏÑ±ÌïòÎùº:
        
        Ï∂úÎ†•ÏùÄ Î∞òÎìúÏãú Îã§Ïùå ÌòïÏãùÏùÑ Îî∞ÎùºÏïº ÌïúÎã§.
        
        ÌïµÏã¨ Ï£ºÏ†ú: 
        """
    )
    return prompt
    
# ÌîÑÎ°¨ÌîÑÌä∏ Ïª¨Îüº ÏÉùÏÑ± (Î†àÏù¥Î∏îÏùÄ placeholder)
df_all["prompt"] = df_all.apply(
    lambda row: build_prompt(
        code_value=row["code"],
        code_vector=row["code_str"],
        ohlcv_summary=row["ohlcv_summary"],
    ),
    axis=1,
)

# Ïã§Ï†ú ÌîÑÎ°úÏ†ùÌä∏ÏóêÏÑúÎäî ÏïÑÎûò completionÏùÑ ÏÇ¨Îûå/teacher LLMÏúºÎ°ú Ï±ÑÏõåÎÑ£Ïñ¥Ïïº Ìï®
# silver labelÏùÑ llm apiÎ°ú ÏÇ¨Ïö©ÌïòÍ±∞ÎÇò ÏÇ¨Îûå ÏÜêÏúºÎ°ú gole labelÏùÑ Î∂ôÏó¨ ÎÑ£Ïñ¥Ïïº Ìï®
df_all["completion"] = df_all["text"]

print("[ALL with prompt/completion] head:\n",
      df_all[["date", "code", "prompt", "completion"]].head())



# import boto3
import json

def export_jsonl(df: pd.DataFrame, out_path: str) -> None:
    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        for row in df.itertuples():
            rec = {
                "prompt": getattr(row, "prompt"),
                "completion": getattr(row, "completion"),
                "date": str(getattr(row, "date")),
            }
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    print(f"Saved JSONL to {out_path}")

export_jsonl(df_all, OUTPUT_JSONL_LOCAL_PATH)

# upload to s3 bucket
# s3 = boto3.client("s3")
# s3.upload_file(OUTPUT_JSONL_LOCAL_PATH, BUCKET, OUTPUT_JSONL_S3_KEY)

# print(f"Uploaded to s3://{BUCKET}/{OUTPUT_JSONL_S3_KEY}")





import os
import json
from dataclasses import dataclass
from typing import Dict, List

import random
import numpy as np
import torch
from torch.utils.data import Dataset

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    default_data_collator,
)

from peft import LoraConfig, get_peft_model



@dataclass
class TrainConfig:
    # 1) Î™®Îç∏ & Îç∞Ïù¥ÌÑ∞
    model_name_or_path: str = "meta-llama/Meta-Llama-3-8B-Instruct"  # ÏõêÌïòÎäî Î≤†Ïù¥Ïä§ LLM Ïù¥Î¶Ñ
    train_jsonl: str = "prepared_data/train.jsonl"          # ÌïôÏäµÏãúÌÇ¨ json prompt

    # Í≤∞Í≥º Ï†ÄÏû• ÏúÑÏπò: EBS Î≥ºÎ•® ÎÇ¥ ÌòÑÏû¨ ÏûëÏóÖ ÎîîÎ†âÌÜ†Î¶¨ Í∏∞Ï§Ä "./model"
    output_dir: str = "./model"

    # 2) ÌÜ†ÌÅ¨ÎÇòÏù¥Ï¶à & Í∏∏Ïù¥
    max_length: int = 512

    # 3) ÌïôÏäµ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞
    per_device_train_batch_size: int = 1
    gradient_accumulation_steps: int = 16
    num_train_epochs: int = 3
    learning_rate: float = 2e-4
    warmup_ratio: float = 0.03

    # 4) Î°úÍπÖ & Ï†ÄÏû•
    logging_steps: int = 1
    save_steps: int = 500
    seed: int = 42
    # 5) Í∏∞ÌÉÄ
    dataloader_num_workers: int = 2  # ColabÏù¥Î©¥ 0~2 Ï†ïÎèÑÎ°ú Ïú†ÏßÄ

cfg = TrainConfig()
cfg



def resolve_jsonl_path(path: str) -> str:
    """
    - s3://bucket/key ÌòïÏãùÏù¥Î©¥ /tmp/train.jsonl Î°ú Îã§Ïö¥Î°úÎìú ÌõÑ Ìï¥Îãπ Í≤ΩÎ°ú Î∞òÌôò
    - Í∑∏ Ïô∏ÏóêÎäî Î°úÏª¨ Í≤ΩÎ°ú Í∑∏ÎåÄÎ°ú Î∞òÌôò
    """
    if path.startswith("s3://"):
        no_scheme = path[5:]
        bucket, key = no_scheme.split("/", 1)

        local_path = "/tmp/train.jsonl"
        os.makedirs(os.path.dirname(local_path), exist_ok=True)

        s3 = boto3.client("s3")
        s3.download_file(bucket, key, local_path)
        print(f"[INFO] downloaded {path} -> {local_path}")
        return local_path
    else:
        return path


def seed_everything(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


seed_everything(cfg.seed)



class JsonlSftDataset(Dataset):
    """
    SFTÏö© JSONL Dataset.
    Í∞Å rowÎäî {"prompt": "...", "completion": "..."} ÌòïÌÉúÏó¨Ïïº Ìï®.
    Î∞òÌôòÎêòÎäî input_ids, attention_mask, labelsÎäî Î™®Îëê ÎèôÏùº Í∏∏Ïù¥Î•º Î≥¥Ïû•Ìï¥Ïïº ÌïúÎã§.
    """

    def __init__(self, jsonl_path, tokenizer, max_length=1024):
        self.samples = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                if not line.strip():
                    continue
                obj = json.loads(line)
                self.samples.append(obj)

        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        item = self.samples[idx]
        prompt = " ".join(str(item["prompt"]).split())
        completion = " ".join(str(item["completion"]).split())

        # 1) ÌîÑÎ°¨ÌîÑÌä∏ + completion Ìï©ÏπòÍ∏∞
        full_text = prompt + completion

        # 2) tokenizerÎ°ú ÏùºÍ¥Ñ encoding (truncation=True)
        enc = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            add_special_tokens=True,
        )

        input_ids = enc["input_ids"]
        attn_mask = enc["attention_mask"]

        # 3) promptÎßå tokenizeÌï¥ÏÑú Í∏∏Ïù¥ ÌååÏïÖ
        prompt_ids = self.tokenizer(
            prompt,
            add_special_tokens=False,
        )["input_ids"]

        # BOS ÌÜ†ÌÅ∞ Î≥¥Ï†ï
        bos_extra = 1 if (len(input_ids) > 0 and input_ids[0] == self.tokenizer.bos_token_id) else 0

        prompt_len = len(prompt_ids) + bos_extra

        # 4) labels = input_ids Î≥µÏÇ¨
        labels = input_ids.copy()

        # prompt Íµ¨Í∞Ñ -100
        for i in range(prompt_len):
            if i < len(labels):
                labels[i] = -100

        # ‚Äª Ïó¨Í∏∞ÏÑú Ï§ëÏöîÌïú Î∂ÄÎ∂Ñ: 
        # input_ids, attention_mask, labels Î™®Îëê Ï†ïÌôïÌûà ÎèôÏùº Í∏∏Ïù¥
        # collatorÍ∞Ä ÎÇòÏ§ëÏóê batch padding Ìï† Í≤ÉÏù¥ÎØÄÎ°ú DatasetÏóêÏÑúÎäî Í∏∏Ïù¥Î•º ÎßûÏ∂îÍ∏∞Îßå ÌïòÎ©¥ ÎêúÎã§.

        assert len(input_ids) == len(labels), f"label mismatch: {len(input_ids)} vs {len(labels)}"

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attn_mask, dtype=torch.long),
            "labels": torch.tensor(labels, dtype=torch.long),
        }



from huggingface_hub import login
from huggingface_hub import HfApi
from huggingface_hub import hf_hub_download

api = HfApi()
login(token="")

try:
    hf_hub_download(
        repo_id="meta-llama/Meta-Llama-3-8B-Instruct",
        filename="config.json"
    )
    print("Access OK!")
except Exception as e:
    print("Access error:", e)

"""
import requests

HF_TOKEN = "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"   # ÎÑàÏùò Í≤É ÎÑ£Í∏∞
headers = {"Authorization": f"Bearer {HF_TOKEN}"}

url = "https://huggingface.co/api/models/meta-llama/Meta-Llama-3-8B-Instruct"
res = requests.get(url, headers=headers)

if res.status_code == 200:
    data = res.json()
    if data.get("gated", False):
        print("This repo is gated. You need approval from HuggingFace.")
    else:
        print("Repo is public or you have access!")
else:
    print("Error:", res.text)
"""



# 1) ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä
tokenizer = AutoTokenizer.from_pretrained(
    cfg.model_name_or_path,
    use_fast=True,
)

# pad ÌÜ†ÌÅ∞ ÏÑ∏ÌåÖ (ÏóÜÎäî Î™®Îç∏Îì§ ÎåÄÎπÑ)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"


# 2) Î™®Îç∏ Î°úÎìú
model = AutoModelForCausalLM.from_pretrained(
    cfg.model_name_or_path,
    torch_dtype=torch.bfloat16,
    device_map="cuda",  # GPU ÏûàÏúºÎ©¥ GPU, ÏóÜÏúºÎ©¥ CPU
)

model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = False  # ÌïôÏäµ ÏãúÏóêÎäî False Í∂åÏû•

# 3) LoRA ÏÑ§Ï†ï
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    # LLaMA Í≥ÑÏó¥ Í∏∞Ï§Ä. Îã§Î•∏ Î™®Îç∏Ïù¥Î©¥ target_modules Ïù¥Î¶ÑÎßå Î∞îÍøîÏ£ºÎ©¥ Îê®.
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


import torch
from torch.nn.utils.rnn import pad_sequence

# pad tokenÏù¥ ÏóÜÎäî Î™®Îç∏Ïù¥Î©¥ eosÎ•º padÎ°ú Ïû¨ÏÇ¨Ïö©
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id

def sft_collate_fn(batch):
    """
    batch: list of {"input_ids": 1D tensor, "attention_mask": 1D tensor, "labels": 1D tensor}
    Î•º Î∞õÏïÑÏÑú, paddingÎêú Î∞∞ÏπòÎ•º Î∞òÌôò.
    """

    input_ids_list = [item["input_ids"] for item in batch]
    attention_mask_list = [item["attention_mask"] for item in batch]
    labels_list = [item["labels"] for item in batch]

    # Î∞∞Ïπò ÏïàÏóêÏÑú Í∞ÄÏû• Í∏¥ Í∏∏Ïù¥Ïóê ÎßûÏ∂∞ Ìå®Îî©
    input_ids_padded = pad_sequence(
        input_ids_list,
        batch_first=True,
        padding_value=tokenizer.pad_token_id,
    )

    attention_mask_padded = pad_sequence(
        attention_mask_list,
        batch_first=True,
        padding_value=0,          # Ìå®Îî© ÏúÑÏπòÎäî 0
    )

    labels_padded = pad_sequence(
        labels_list,
        batch_first=True,
        padding_value=-100,       # lossÏóêÏÑú Î¨¥ÏãúÎê† Í∞í
    )

    return {
        "input_ids": input_ids_padded,
        "attention_mask": attention_mask_padded,
        "labels": labels_padded,
    }



# S3 ÎòêÎäî Î°úÏª¨ Í≤ΩÎ°úÎ•º Ïã§Ï†ú Î°úÏª¨ ÌååÏùºÎ°ú resolve
train_jsonl_local = resolve_jsonl_path(cfg.train_jsonl)
print("Train JSONL local path:", train_jsonl_local)

# Dataset ÏÉùÏÑ±
from torch.utils.data import Subset
from transformers import DataCollatorWithPadding

# 1) Ï†ÑÏ≤¥ dataset ÏÉùÏÑ±
full_dataset = JsonlSftDataset(
    jsonl_path=train_jsonl_local,
    tokenizer=tokenizer,
    max_length=cfg.max_length,
)

# 2) train / val split (Ïòà: 90% / 10%)
n = len(full_dataset)
val_ratio = 0.1
val_size = max(1, int(n * val_ratio))

indices = list(range(n))
import random
random.shuffle(indices)

val_indices = indices[:val_size]
train_indices = indices[val_size:]

train_dataset = Subset(full_dataset, train_indices)
eval_dataset = Subset(full_dataset, val_indices)

print(f"Total samples: {n}, train: {len(train_dataset)}, val: {len(eval_dataset)}")



import transformers
print(transformers.__version__)



# trainer
from transformers import TrainingArguments

os.makedirs(cfg.output_dir, exist_ok=True)

training_args = TrainingArguments(
    output_dir=cfg.output_dir,
    overwrite_output_dir=True,

    num_train_epochs=cfg.num_train_epochs,
    per_device_train_batch_size=cfg.per_device_train_batch_size,
    gradient_accumulation_steps=cfg.gradient_accumulation_steps,
    learning_rate=cfg.learning_rate,
    warmup_ratio=cfg.warmup_ratio,

    logging_steps=cfg.logging_steps,
    save_steps=cfg.save_steps,
    save_total_limit=3,

    fp16=False,
    bf16=True,

    seed=cfg.seed,
    report_to=[],
    dataloader_num_workers=cfg.dataloader_num_workers,

    # Ï∂îÍ∞Ä: epochÎßàÎã§ eval_loss Í≥ÑÏÇ∞
    eval_strategy="epoch",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,   # ‚Üê Ï∂îÍ∞Ä
    data_collator=sft_collate_fn,
)

print("Trainer ready.")




for i in range(0, len(train_dataset)):
    
    sample = train_dataset[i]
    print(len(sample["input_ids"]), len(sample["labels"]))


trainer.train()


import os
from datetime import datetime

def save_metrics_with_timestamp(df, prefix, out_dir="./logs"):
    # 1) ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
    os.makedirs(out_dir, exist_ok=True)

    # 2) ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ ÏÉùÏÑ±
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")

    # 3) ÌååÏùº Í≤ΩÎ°ú ÏÉùÏÑ±
    filename = f"{prefix}_{ts}.csv"
    path = os.path.join(out_dir, filename)

    # 4) CSV Ï†ÄÏû•
    df.to_csv(path, index=False, encoding="utf-8-sig")

    print(f"Saved metrics to: {path}")
    return path


#epcohÎ≥Ñ eval_lossÏôÄ perplexity Ï†ïÎ¶¨
import math
import pandas as pd

log_history = trainer.state.log_history

rows = []
for log in log_history:
    # eval Îã®Í≥Ñ Î°úÍ∑∏Îßå Í≥®ÎùºÏÑú ÏÇ¨Ïö©
    if "eval_loss" in log:
        epoch = log.get("epoch", None)
        eval_loss = log["eval_loss"]
        perplexity = math.exp(eval_loss)
        rows.append({
            "epoch": epoch,
            "eval_loss": eval_loss,
            "perplexity": perplexity,
        })

df_metrics = pd.DataFrame(rows)
filename = "lora_metrics"
save_metrics_with_timestamp(df_metrics, filename)
print(df_metrics)



trainer.save_model(cfg.output_dir)      # LoRA Ïñ¥ÎåëÌÑ∞ Ìè¨Ìï®Ìïú Î™®Îç∏ Ï†ÄÏû•
tokenizer.save_pretrained(cfg.output_dir)

print("LoRA fine-tuning ÏôÑÎ£å. Ï†ÄÏû• ÏúÑÏπò:", cfg.output_dir)






import pandas as pd

df_test = pd.read_csv("prepared_data/test_embedding.csv")
df_price_with_summary = pd.read_csv("prepared_data/df_price_with_summary.csv")


df_test_sample = df_test.sample(30)
df_test_sample


df_test_sample['date'] = pd.to_datetime(df_test_sample['date']).dt.date
df_price_with_summary['date'] = pd.to_datetime(df_price_with_summary['date']).dt.date

df_test_sample = pd.merge(
    df_test_sample,
    df_price_with_summary,                # VQ ÏΩîÎìú + Í∞ÄÍ≤© ÏöîÏïΩ
    on="date",
    how="left",
)


def code_to_str(row):
    z_values = [row[f"z_{i}"] for i in range(8)]
    z_str = ", ".join([f"{v:.6f}" for v in z_values])
    return f"{z_str}"

df_test_sample["code_str"] = df_test_sample.apply(code_to_str, axis=1)


df_test_sample


from peft import PeftModel

# Î≤†Ïù¥Ïä§ Î™®Îç∏ Îã§Ïãú Î°úÎìú
base_model = AutoModelForCausalLM.from_pretrained(
    cfg.model_name_or_path,
    torch_dtype=torch.float16,
    device_map="cuda",
)
base_model.config.pad_token_id = tokenizer.pad_token_id
base_model.config.use_cache = True  # inferenceÏóêÏÑúÎäî TrueÎ°ú ÏºúÎèÑ Îê©ÎãàÎã§.

lora_model = PeftModel.from_pretrained(
    base_model,
    cfg.output_dir,
)
lora_model.eval()



def generate_comment(prompt_text: str, max_new_tokens: int = 200):
    inputs = tokenizer(
        prompt_text,
        return_tensors="pt",
        padding=True,
        truncation=False,
    ).to(lora_model.device)\

    with torch.no_grad():
        outputs = lora_model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            top_p=0.9,
            temperature=0.5,
            pad_token_id=tokenizer.pad_token_id,
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def inference_df(row):
    code = row['code']
    code_str = row['code_str']
    ohlcv_summary = row['ohlcv_summary']

    test_prompt = f"""
    ### Ï£ºÏñ¥ÏßÑ Ï†ïÎ≥¥
    ÏïÑÎûòÎäî ÌäπÏ†ï Í∏∞Í∞Ñ ÎèôÏïàÏùò ÏãúÏû• Ìå®ÌÑ¥ÏùÑ ÎÇòÌÉÄÎÇ¥Îäî VQ ÏΩîÎìúÏôÄ Ìï¥Îãπ Í∏∞Í∞ÑÏùò OHLCV Îç∞Ïù¥ÌÑ∞Ïù¥Îã§.
    
    ### ÏΩîÎìúÎ∂Å
    {code}
    
    ### ÏΩîÎìúÎ∂Å Î≤°ÌÑ∞
    {code_str}
    
    ### OHLCV ÏãúÍ≥ÑÏó¥
    {ohlcv_summary}
    
    ### ÏßÄÏãúÎ¨∏
    ÎãπÏã†ÏùÄ ÎØ∏Íµ≠ Ï£ºÏãù ÏãúÏû•ÏùÑ Î∂ÑÏÑùÌïòÎäî Ï†ÑÎ¨∏ Ïï†ÎÑêÎ¶¨Ïä§Ìä∏Îã§.
    ÎùºÎ≤®Ïóê Ìè¨Ìï®Îêú ÏÇ¨Í±¥Ïùò ÏÑ±Í≤©(ÏÑ±Ïû•, Î∂ÄÏßÑ, Íµ¨Ï°∞Ï°∞Ï†ï, Î∂àÌôïÏã§ÏÑ± Îì±)ÏùÑ ÏûÑÏùò Î≥ÄÌòïÌïòÍ±∞ÎÇò ÏïΩÌôîÌïòÏßÄ ÏïäÎäîÎã§.
    Ï∂úÎ†•ÏùÄ Î∞òÎìúÏãú Îëê Î¨∏Ïû•ÏúºÎ°ú Íµ¨ÏÑ±ÌïúÎã§.
    Ï≤´ Î¨∏Ïû•ÏùÄ Î∞òÎìúÏãú "ÌïµÏã¨ Ï£ºÏ†ú:"Î°ú ÏãúÏûëÌïòÎ©∞ ÌïòÎÇòÏùò Î¨∏Ïû•ÏúºÎ°ú ÎÅùÎÇ∏Îã§.
    Îëê Î≤àÏß∏ Î¨∏Ïû•ÏùÄ Î∂ÑÏÑù ÎÇ¥Ïö©ÏùÑ Îã¥Îêò ÌïòÎÇòÏùò Î¨∏Ïû•ÏúºÎ°ú ÎÅùÎÇ∏Îã§.
    Î∂àÌïÑÏöîÌïú Î∞òÎ≥µÏùÑ Í∏àÏßÄÌïòÎ©∞, Î¨∏Ïû•ÏùÑ Ï†àÎåÄÎ°ú ÎäòÏù¥Í±∞ÎÇò Ïù¥Ïñ¥Î∂ôÏù¥ÏßÄ ÏïäÎäîÎã§.
    Î¨∏Ïû•ÏùÄ ÎÅùÎß∫ÏùåÏù¥ ÌôïÏã§Ìï¥Ïïº ÌïòÎ©∞ Ï∂úÎ†•ÏùÄ Î∞òÎìúÏãú Ï¥ù Îëê Î¨∏Ïû•ÏúºÎ°ú Íµ¨ÏÑ±ÌïúÎã§.
    
    ### Ï∂úÎ†•
    """
    return generate_comment(test_prompt)


df_test_sample["inference"] = df_test_sample.apply(inference_df, axis=1)


print(df_test_sample["inference"][3])


import re

def extract_two_sentences(row):

    output_text = row["inference"]
    
    if "### Ï∂úÎ†•" in output_text:
        output_text = output_text.split("### Ï∂úÎ†•", 1)[1].strip()

    sentences = re.split(r'(?<=[\.!?Îã§|Ïöî|ÏäµÎãàÎã§])\s+', output_text)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]

    if len(sentences) < 2:
        return output_text

    final = sentences[0].rstrip(".") + ".\n" + sentences[1].rstrip(".") + "."

    return final


df_test_sample["extract_inf"] = df_test_sample.apply(extract_two_sentences, axis=1)


df_test_sample.to_csv("inference.csv")





ohlcv_data = pd.read_csv('spy_2023_2024.csv')


ohlcv_data


origin_indices = [4862]


import math
from plotly.subplots import make_subplots
import plotly.graph_objects as go

data_n = len(origin_indices)
rows = math.ceil(data_n / 2)

fig = make_subplots(
    rows = rows, cols = 2,
    shared_yaxes=False,
    horizontal_spacing=0.05,
    vertical_spacing=0.06
)

for i in range(data_n):
  target = origin_indices[i]
  data_object = ohlcv_data.iloc[target: (target + 13)]

  fig.add_trace(go.Candlestick(x=data_object['date'],open=data_object['1. open'],high=data_object['2. high'],low=data_object['3. low'],close=data_object['4. close']), row=i // 2 + 1, col=i % 2 + 1)

fig.update_xaxes(rangeslider_visible=False)
fig.update_layout(
    height=rows * 260,
    margin=dict(t=30, b=30, l=40, r=20)
)

fig.show()




